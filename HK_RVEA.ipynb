{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc49a84",
   "metadata": {
    "id": "4dba3936-96eb-4f1f-8932-d2ed9995326e"
   },
   "source": [
    "# HK-RVEA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005690a-40ce-4d54-9939-2602138ed502",
   "metadata": {
    "id": "5005690a-40ce-4d54-9939-2602138ed502"
   },
   "source": [
    "## Part 0: Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf23304-03b7-4bb4-b36d-dc9ddbf7b06c",
   "metadata": {
    "id": "ddf23304-03b7-4bb4-b36d-dc9ddbf7b06c"
   },
   "source": [
    "### Part 0.1: Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88282af-3181-4399-ab47-3102ad5452f4",
   "metadata": {
    "id": "d88282af-3181-4399-ab47-3102ad5452f4"
   },
   "outputs": [],
   "source": [
    "#stable release of pymoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901638bc-aae0-413b-bd66-8f74c66816db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "901638bc-aae0-413b-bd66-8f74c66816db",
    "outputId": "c8079b7c-2036-466a-84cc-ed4dec8bc797",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U pymoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640ce33e-3174-4ffb-81cf-6de6b3e86d41",
   "metadata": {
    "id": "640ce33e-3174-4ffb-81cf-6de6b3e86d41"
   },
   "outputs": [],
   "source": [
    "#release candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a860a81-f4f1-4ca6-9889-3032930772fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a860a81-f4f1-4ca6-9889-3032930772fe",
    "outputId": "e1a2bcf7-3d4c-4b1e-c8b6-2e09d320ba2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --pre -U pymoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9e496-deae-4dcc-bc18-06b38d3c572b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6a9e496-deae-4dcc-bc18-06b38d3c572b",
    "outputId": "b86f51de-0a70-4d6c-9921-bce511bc3dd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install Platypus-Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xjgbR_d74cr0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjgbR_d74cr0",
    "outputId": "f0ea0acb-21a1-4a64-a6b2-b77d25774428",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RU33T5v6F6T_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RU33T5v6F6T_",
    "outputId": "73397c16-6f26-4001-c8e5-9b1f4eac9cc9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pygmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a38e79-571f-4544-8f07-73a7edea5d49",
   "metadata": {
    "id": "61a38e79-571f-4544-8f07-73a7edea5d49"
   },
   "source": [
    "### Part 0.2: Library Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402dd2c4-c79c-4176-be04-971d9e247125",
   "metadata": {
    "id": "402dd2c4-c79c-4176-be04-971d9e247125"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import randint, rand, randn\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import comb\n",
    "from scipy.stats.qmc import LatinHypercube\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.cluster import KMeans\n",
    "from pymoo.indicators.hv import HV\n",
    "from pygmo import hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ijfPFxASPVYp",
   "metadata": {
    "id": "ijfPFxASPVYp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "import pygmo\n",
    "\n",
    "import platypus as plat              # multi-objective optimisation framework\n",
    "import pygmo as pg                   # multi-objective optimisation framework\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb171208-0ecf-450b-bda5-8751d9f3753e",
   "metadata": {
    "id": "cb171208-0ecf-450b-bda5-8751d9f3753e"
   },
   "source": [
    "## Part 1: Input Data and Termination Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ac0e0-d5b9-4484-8f5a-16c6291ae435",
   "metadata": {
    "id": "e67ac0e0-d5b9-4484-8f5a-16c6291ae435"
   },
   "outputs": [],
   "source": [
    "id_nex = 1 # objective no. for inexpensive objective function --> f1 (in fist column)-->index 0\n",
    "id_ex = 2 # objective no. for expensive objective function --> f2 (in second column)-->index 1\n",
    "Problems = ['DTLZ2'] # 'DTLZ2','DTLZ5', 'cmmx1' # varies when using different problem --> DTLZ5 or cm-OneMax...\n",
    "\n",
    "# Input\n",
    "# M = 2 # number of objectives --> 未來應該改成no_obj = [2,3,4,5....]\n",
    "no_obj = [2] # 會assign給M\n",
    "no_var = 20 # number of decision variables\n",
    "Bounds = np.array([[1] * no_var, [0] * no_var]) # Bounds with 10 columns and 2 rows\n",
    "\n",
    "# Termination criterion\n",
    "Latencies = np.array([1,2,4,8,10,15]) # 1,2,4,8,10,15 <-Latency value (ratio of computation time of one evaluation of expensive funciton i to one evaluation of inexpensive function j)\n",
    "Max_FE_nex = 1000 # maximum number of funciton evaluations for inexpensive objective functions -> how is this number defined?\n",
    "\n",
    "# for sensitivity analysis\n",
    "population_size = 50 # remember to modify generate_initial_data too\n",
    "cross_prob = 0.8 # base = 0.8\n",
    "mut_prob = 0.1 # base = 0.1 = (1/no_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae77254-ba93-4453-9667-013862d92264",
   "metadata": {
    "id": "5ae77254-ba93-4453-9667-013862d92264"
   },
   "source": [
    "## Part 2: HK-RVEA Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bf9d4-d7c4-4480-91a6-f33b9fd680e0",
   "metadata": {
    "id": "ef2bf9d4-d7c4-4480-91a6-f33b9fd680e0"
   },
   "source": [
    "### Part 2.1: Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xvQVqbUG82HV",
   "metadata": {
    "id": "xvQVqbUG82HV"
   },
   "source": [
    "#### generate initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51184f-c422-4eac-973a-d2c4309f9010",
   "metadata": {
    "id": "9a51184f-c422-4eac-973a-d2c4309f9010"
   },
   "outputs": [],
   "source": [
    "def generate_initial_data(Bounds):\n",
    "\n",
    "    no_var = Bounds.shape[1] # return the number of columns the array \"Bounds\" has = no_var\n",
    "    sampler = qmc.LatinHypercube(d=no_var)\n",
    "    # sampler = qmc.LatinHypercube(d=no_var, seed = 42) # 這會使得每次initial P都是一樣的\n",
    "    sample = sampler.random(n=50).reshape(50,no_var)\n",
    "    round_sample = np.round(sample, 5)\n",
    "    ub = Bounds[0, :]  # Get the first row of Bounds\n",
    "    lb = Bounds[1, :]  # Get the second row of Bounds\n",
    "    scaled_sample = lb + (round_sample * (ub - lb)) # scale the bounds: lb+(ub-lb)*samples(P) generated by LH\n",
    "    return np.array(scaled_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NfB5Okan8qzd",
   "metadata": {
    "id": "NfB5Okan8qzd"
   },
   "source": [
    "#### p_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c42c5d-6b29-4d45-946b-6e3435238ee2",
   "metadata": {
    "id": "72c42c5d-6b29-4d45-946b-6e3435238ee2"
   },
   "outputs": [],
   "source": [
    "def P_objective(Operation, Problem, M, Input): #(Operation='value', Problem='DTLZ2', M=2, Input=Population) # population is a shape of 50x10\n",
    "\n",
    "    k = len(Problem) - 1 # the problem is \"DTLZ2\" --> k=4\n",
    "\n",
    "    while not Problem[k].isdigit(): # Problem[4].isdigit() returns True -->while not True = while False-->this loop will never be executed as long as the problem is DTLZ2\n",
    "        k -= 1\n",
    "\n",
    "    if Problem[:k] == 'DTLZ' or 'cmmx':\n",
    "        Output,Boundary,Coding = P_DTLZ(Operation, Problem, M, Input)\n",
    "    else:\n",
    "        raise Exception('please provide a DTLZ problem')\n",
    "\n",
    "    return Output, Boundary, Coding\n",
    "\n",
    "# output: objective FunctionValue for a set of input solutions (P)\n",
    "# boundary: bounds of decision variables;\n",
    "# coding: encoding of the decision variables, which represent decision variables in a form that can be processed by an optimisation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pdQnLSg385W9",
   "metadata": {
    "id": "pdQnLSg385W9"
   },
   "source": [
    "#### p_dtlz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b780ed6-f8a6-4dda-85a5-144a6c5a54e5",
   "metadata": {
    "id": "4b780ed6-f8a6-4dda-85a5-144a6c5a54e5"
   },
   "outputs": [],
   "source": [
    "# 評估objective以得出FunctionValue(output)的function\n",
    "# the formula for DTLZ2 and 5 would have to modify if M is not 2. the formula for FunctionValue will be differernt when there are more than 2 objectives!\n",
    "def P_DTLZ(Operation, Problem, M, Input): #(Operation='value', Problem='DTLZ2', M=2, and Input=Population)\n",
    "\n",
    "    global K\n",
    "    Boundary = np.nan\n",
    "    Coding = np.nan\n",
    "\n",
    "    N = Input.shape[0]  # Number of rows in the Input array\n",
    "    FunctionValue = np.zeros((N, M)) # initialise it and ensure it has the same shape with Population\n",
    "\n",
    "    if Operation == 'init':\n",
    "        # k = max([i for i, c in enumerate(Problem) if not c.isdigit()])\n",
    "        k = len(Problem) - 1 # 上面那句改成這樣\n",
    "        K = (no_var - M + 1) * np.ones((7, 1)) # 一個shape(7x1)的array 裡面都是10-M+1 (10 = no_var)\n",
    "        K = K[int(Problem[k:])] # 應為9\n",
    "        D = int(M + K - 1) # D = 2 + 9 - 1 = 10\n",
    "        MaxValue = np.ones((1, D)) # [1,1,1,1,1,1,1,1,1,1]\n",
    "        MinValue = np.zeros((1, D)) # [0,0,0,0,0,0,0,0,0,0]\n",
    "        Population = np.random.rand(Input.shape[0], D)\n",
    "        Population = Population * np.tile(MaxValue, (Input, 1)) + \\\n",
    "            (1 - Population) * np.tile(MinValue, (Input, 1))\n",
    "\n",
    "        Output = Population\n",
    "        Boundary = np.vstack((MaxValue, MinValue))\n",
    "        Coding = 'Real'\n",
    "\n",
    "    elif Operation == 'value': # 計算FunctionValue\n",
    "        Population = Input\n",
    "        K = no_var - M + 1 ### xM vectors是由\"最後\"K個variables組成的 K = no_var - M + 1 = 9\n",
    "        FunctionValue = np.zeros((Population.shape[0], M))\n",
    "\n",
    "        if Problem == 'DTLZ2':\n",
    "          g = np.sum((Population[:,M-1:] - 0.5)**2, axis = 1)\n",
    "          for i in range(M):\n",
    "            if i == 0: # f0 = F_nex in HK-RVEA\n",
    "              FunctionValue[:,i] = ((1 + g) *\n",
    "                np.prod(np.cos(0.5 * np.pi * Population[:,:M-1-i]), axis = 1))\n",
    "\n",
    "            elif i == 1: # f1 = F_ex in HK-RVEA -> # 用f0除以(最後一個var的cos)並乘以第M-2個var的sine\n",
    "              FunctionValue[:,i] = (FunctionValue[:,i-1] /\n",
    "                np.prod(np.cos(0.5 * np.pi * Population[:,M-1-i].reshape(-1,1)), axis = 1) *\n",
    "                np.prod(np.sin(0.5 * np.pi * Population[:,M-i-1].reshape(-1,1)), axis = 1))\n",
    "\n",
    "            else: # 用前一個f除以(最後一個cosine跟sin)並乘以第M-i-1個sine\n",
    "              FunctionValue[:,i] = (FunctionValue[:,i-1] /\n",
    "                (np.prod(np.cos(0.5 * np.pi * Population[:,M-i-1].reshape(-1,1)), axis = 1) *\n",
    "                np.prod(np.sin(0.5 * np.pi * Population[:,M-i].reshape(-1,1)), axis = 1)) *\n",
    "                np.prod(np.sin(0.5 * np.pi * Population[:,M-i-1].reshape(-1,1)), axis = 1))\n",
    "\n",
    "        elif Problem == 'DTLZ5':\n",
    "          g = np.sum((Population[:,M-1:] - 0.5)**2, axis = 1)\n",
    "          theta = np.zeros((Population.shape[0], M))\n",
    "          # theta[:,0] = (0.5 * np.pi * Population[:,0])\n",
    "          # theta[:,1:] = (np.pi / (4 * (1 + g))) * (1 + 2 * g * Population[:,1:M-1])\n",
    "\n",
    "          for i in range(M):\n",
    "            if i == 0: # f0 = F_nex in HK-RVEA\n",
    "              theta[:,i] = (0.5 * np.pi * Population[:,0])\n",
    "              FunctionValue[:,i] = ((1 + g) *\n",
    "                np.prod(np.cos(0.5 * np.pi * theta[:,:M-1-i]), axis = 1))\n",
    "            elif i == 1: # f1 = F_ex in HK-RVEA -> # 用f0除以(最後一個var的cos)並乘以第M-2個var的sine\n",
    "              theta[:,i] = (np.pi / (4 * (1 + g))) * (1 + 2 * g * Population[:,i])\n",
    "              FunctionValue[:,i] = (FunctionValue[:,i-1] /\n",
    "                np.prod(np.cos(0.5 * np.pi * theta[:,M-1-i]).reshape(-1,1), axis = 1) *\n",
    "                np.prod(np.sin(0.5 * np.pi * theta[:,M-i-1]).reshape(-1,1), axis = 1))\n",
    "            else: # 用前一個f除以(最後一個cosine跟sin)並乘以第M-i-1個sine\n",
    "              theta[:,i] = (np.pi / (4 * (1 + g))) * (1 + 2 * g * Population[:,i])\n",
    "              FunctionValue[:,i] = (FunctionValue[:,i-1] /\n",
    "                (np.prod(np.cos(0.5 * np.pi * theta[:,M-i-1].reshape(-1,1)), axis = 1) *\n",
    "                np.prod(np.sin(0.5 * np.pi * theta[:,M-i].reshape(-1,1)), axis = 1)) *\n",
    "                np.prod(np.sin(0.5 * np.pi * theta[:,M-i-1].reshape(-1,1)), axis = 1))\n",
    "\n",
    "        # https://github.com/fcampelo/DEMO/blob/master/Octave-Matlab/DTLZ/dtlz5.m Github Matlab Theta_1的定義\n",
    "\n",
    "        elif Problem == 'cmmx1': # cm_OneMax\n",
    "          corr = 0.5 # or -0.5\n",
    "          # Calculate mapi based on correlation corr\n",
    "          prob_zero = (1 + corr) / 2\n",
    "          map = np.random.choice([0, 1], size=(1, 10), p=[prob_zero, 1 - prob_zero])\n",
    "          # print(\"map\")\n",
    "          # print(map)\n",
    "\n",
    "          # Calculate yi = |xi - mapi|\n",
    "          y = np.abs(Population - map)\n",
    "          # print(\"yi\")\n",
    "          # print(y)\n",
    "\n",
    "          n_x = np.sum(Population, axis =1)\n",
    "          n_y = np.sum(y, axis = 1)\n",
    "          # print('nx')\n",
    "          # print(n_x)\n",
    "          # print('ny')\n",
    "          # print(n_y)\n",
    "\n",
    "          FunctionValue[:,0] = n_x\n",
    "          FunctionValue[:,1] = n_y\n",
    "          # print(\"functionvalue\")\n",
    "          # print(FunctionValue)\n",
    "        Output = FunctionValue\n",
    "\n",
    "\n",
    "\n",
    "    elif Operation == 'true':\n",
    "        if Problem == 'DTLZ2':\n",
    "            Population = T_uniform(Input, M) \n",
    "            for i in range(Population.shape[0]):\n",
    "                Population[i, :] = Population[i, :] / \\\n",
    "                    np.linalg.norm(Population[i, :])\n",
    "        elif Problem == 'DTLZ5':\n",
    "            Population = T_uniform(Input, M - 1) # M or M-1??\n",
    "            for i in range(Population.shape[0]):\n",
    "                Population[i, :] = Population[i, :] / np.linalg.norm(Population[i, :])\n",
    "        Output = Population\n",
    "\n",
    "    return Output, Boundary, Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qUYt4rGU87K1",
   "metadata": {
    "id": "qUYt4rGU87K1"
   },
   "source": [
    "#### t uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8586a3-d27c-4550-8f31-e1f4c61acd5d",
   "metadata": {
    "id": "ce8586a3-d27c-4550-8f31-e1f4c61acd5d"
   },
   "outputs": [],
   "source": [
    "def T_uniform(k, M):\n",
    "\n",
    "    H = int(np.floor((k * np.prod(np.arange(1, M)))**(1 / (M - 1))))\n",
    "    while comb(H + M - 1, M - 1) >= k and H > 0:\n",
    "        H = H - 1\n",
    "    if comb(H + M, M - 1) <= 2 * k or H == 0:\n",
    "        H = H + 1\n",
    "    k = comb(H + M - 1, M - 1)\n",
    "    Temp = comb(np.arange(1, H + M), M - 1, exact=True) - \\\n",
    "        np.tile(np.arange(0, M - 1), (comb(H + M - 1, M - 1), 1)) - 1\n",
    "    W = np.zeros((k, M))\n",
    "    W[:, 0] = Temp[:, 0] - 0\n",
    "    for i in range(1, M - 1):\n",
    "        W[:, i] = Temp[:, i] - Temp[:, i - 1]\n",
    "    W[:, -1] = H - Temp[:, -1]\n",
    "    W = W / H\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ke75WKO389pN",
   "metadata": {
    "id": "Ke75WKO389pN"
   },
   "source": [
    "#### evaluate most expensive obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b81f9-6b02-473a-b833-9f73d85a8175",
   "metadata": {
    "id": "917b81f9-6b02-473a-b833-9f73d85a8175"
   },
   "outputs": [],
   "source": [
    "def evaluate_most_expensive_obj(Population, Problem, id_ex):\n",
    "\n",
    "    F,Boundary,Coding = P_objective('value', Problem, 2, Population)\n",
    "    F = np.array(F) # 跟P同shape的array [f1的FunctionValue,f2的FunctionValue]\n",
    "    f = F[:,int(id_ex)-1].reshape(-1,1)\n",
    "\n",
    "    return f,Boundary,Coding # f = FunctionValue from P_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hBcT5MHw9AMs",
   "metadata": {
    "id": "hBcT5MHw9AMs"
   },
   "source": [
    "#### evaluate least expensive obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd0576-b012-4652-b3aa-ca5ed8941054",
   "metadata": {
    "id": "3dfd0576-b012-4652-b3aa-ca5ed8941054"
   },
   "outputs": [],
   "source": [
    "def evaluate_least_expensive_obj(Population, Problem, id_nex): #id_ex: the objective funciton being evaluated (f1: cheap/ f2: expensive)\n",
    "\n",
    "    F,Boundary,Coding = P_objective('value', Problem, 2, Population)\n",
    "    F = np.array(F)\n",
    "    f = F[:,int(id_nex)-1].reshape(-1,1)\n",
    "\n",
    "    return f,Boundary,Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pch3C2Vu9EE0",
   "metadata": {
    "id": "Pch3C2Vu9EE0"
   },
   "source": [
    "#### optimize least expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf1daf-2de2-411b-a37f-5f8c87f564ad",
   "metadata": {
    "id": "6cbf1daf-2de2-411b-a37f-5f8c87f564ad"
   },
   "outputs": [],
   "source": [
    "def optimize_least_expensive(Population, Bounds, latency, Problem, id_nex):\n",
    "\n",
    "    FE_Max = Population.shape[0] * (latency - 1) \n",
    "\n",
    "    Archive = call_GA(Population, FE_Max, Bounds, Problem, id_nex) # Population?\n",
    "\n",
    "    Pop = Archive[:, :-1] # return all rows and columns excepct the last column in Archive\n",
    "\n",
    "    Fitness = Archive[:, -1] # return all rows and only the last column in Archive\n",
    "\n",
    "\n",
    "    return Pop, Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uj3ERXCA9Gb0",
   "metadata": {
    "id": "Uj3ERXCA9Gb0"
   },
   "source": [
    "#### call ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786600b-d8bb-4c57-9c49-f7390e3e64f0",
   "metadata": {
    "id": "b786600b-d8bb-4c57-9c49-f7390e3e64f0"
   },
   "outputs": [],
   "source": [
    "def call_GA(Population, FE_Max, Boundary, Problem, id_nex):\n",
    "\n",
    "    no_var = Boundary.shape[1]\n",
    "    F,_,_ = P_objective('value', Problem, 2, Population)\n",
    "    F = np.array(F)\n",
    "    FunctionValue = F[:,int(id_nex)-1].reshape(-1,1)\n",
    "\n",
    "    FE = Population.shape[0] # FE is the number of function evaluation, which takes the number of rows of Population\n",
    "\n",
    "\n",
    "    Archive = np.hstack((Population, FunctionValue)) # 50x10 hstack 50x1 --> 左10是P 最後一column為FunctionValue(之後的F_nex)\n",
    "\n",
    "    N = 10 # F_mating's N, P_generator's MaxOffspring parameter\n",
    "\n",
    "    # performing genetic algorithm (GA)\n",
    "    while FE < FE_Max: # FE_Max 在 optimize_least_expensive 裡有先定義過才進到call_GA\n",
    "\n",
    "        MatingPool = F_mating(Population, N) # 產生一個跟Populatoin同size的matingpool 並且都是由population任意塞rows到matingpool(可能會有population重複的rows) 這邊offspring數量是P*(latency-1)\n",
    "\n",
    "        Coding = 'Real'\n",
    "\n",
    "        Offspring = P_generator(MatingPool, Boundary, Coding, N, cross_prob, mut_prob)\n",
    "\n",
    "        Offspring = np.unique(Offspring, axis=0)\n",
    "\n",
    "        Lia = np.isin(Offspring, Archive[:, :no_var]) # check for the uniqueness of each row in 'Offspring' compared to the solutions in the 'Archive' array --> Lia is a boolean array\n",
    "        r_unique = np.where(Lia[:, 0] == False)[0]    # get the rows where the column 0 in Lia is False\n",
    "        Offspring = Offspring[r_unique, :] # those rows not in Archive will be selected and updated to Offspring\n",
    "\n",
    "        F_Values,_,_ = P_objective('value', Problem, 2, Offspring) # get FunctionValue of the updated Offspring\n",
    "        FE = FE + Offspring.shape[0] # size of offspring not the P in the algorihtm\n",
    "\n",
    "        Fitness = F_Values[:, int(id_nex) - 1] # offspring's fitness value\n",
    "        Fitness = Fitness.reshape(-1, 1)\n",
    "\n",
    "\n",
    "        Archive = np.vstack((Archive, np.hstack((Offspring, Fitness.reshape(-1,1)))))\n",
    "        # print(\"Archive shape (after vstack Archive and hstack Offspring, Fitness):\", Archive.shape) #Archive: population 2 columns + FunctionValue 1 column (10,3)\n",
    "                                                                                                    #hstack: offspring 2 columns + Fitness 1 column (10,3)\n",
    "\n",
    "        Population = np.vstack((Population, Offspring)) # new population(with initial P) after evolution\n",
    "        FunctionValue = np.vstack((FunctionValue, Fitness)) # new FunctionValue with initial FunctionValue\n",
    "\n",
    "\n",
    "        if FunctionValue.shape[0] % 2 == 1:\n",
    "            FunctionValue = np.vstack((FunctionValue, FunctionValue[0, :]))\n",
    "            Population = np.vstack((Population, Population[0, :]))\n",
    "        selection = tournamentSelection(2, FunctionValue)\n",
    "        selection = np.array(selection)\n",
    "        selection = np.ravel(selection).astype(int)  # ?\n",
    "        Population = Population[selection, :]\n",
    "        FunctionValue = FunctionValue[selection, :]\n",
    "\n",
    "    return np.array(Archive) # the Archive will be the vstack result of each iteration's Archive (the rows will be extended at each iteration with fixed number of columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d94f2-8907-4b92-94b6-645ffa82d0b9",
   "metadata": {
    "id": "5a2d94f2-8907-4b92-94b6-645ffa82d0b9"
   },
   "source": [
    "#### gentic_operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafdc66-c2d2-4851-9868-434ec2b19132",
   "metadata": {
    "id": "1aafdc66-c2d2-4851-9868-434ec2b19132"
   },
   "outputs": [],
   "source": [
    "def genetic_operation(P, Bounds, latency, Problem, id_nex):\n",
    "\n",
    "    # N = Population.shape[0] * latency - Population.shape[0] # desired number of offspring --> original code\n",
    "    N = P.shape[0] * latency - P.shape[0]\n",
    "\n",
    "    # MatingPool = F_mating(Population, N)\n",
    "    MatingPool = F_mating(P, N)\n",
    "\n",
    "    Coding = 'Real'\n",
    "\n",
    "    Offspring = P_generator(MatingPool, Bounds, Coding, N, cross_prob, mut_prob)\n",
    "\n",
    "    pop = np.vstack((P, Offspring))\n",
    "\n",
    "    F,_,_ = P_objective('value', Problem, 2, pop) # F is fitness value of the combined(updated) pop, which has step 1's P and the Offspring generated based on P\n",
    "    Fitness = F[:, id_nex -1]\n",
    "\n",
    "    return pop, Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96da47c-3655-4655-b263-2c9b497560db",
   "metadata": {
    "id": "b96da47c-3655-4655-b263-2c9b497560db"
   },
   "source": [
    "#### select_solutions_for_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc88dcf-763b-448b-9793-2f7d3f2fd785",
   "metadata": {
    "id": "9cc88dcf-763b-448b-9793-2f7d3f2fd785"
   },
   "outputs": [],
   "source": [
    "def select_solutions_for_archive(P, F_exp, F_nex, id_ex, id_nex):\n",
    "\n",
    "    S = P.shape[0] # S is the number of rows of P generated by lhs\n",
    "    F = np.zeros((S, 2))  # set F, the objective function value, to (s,2) shape with all 0\n",
    "\n",
    "    F_exp = F_exp.reshape(-1,1) \n",
    "    F[:, id_ex - 1] = F_exp[:, 0] # id_ex = 2 --> column 1 in F would be FunctionValue of f2 \n",
    "    F_nex = F_nex.reshape(-1,1) \n",
    "    F[:, id_nex - 1] = F_nex[:S, 0] # id_nex = 1 --> column 0 in F would be FunctionValue of f1\n",
    "\n",
    "    A = np.hstack((P, F)) # A will have 4 columns (populations with 2 objectives, F_exp, F_nex)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0480d6-259b-4e54-854d-cef151b60dd9",
   "metadata": {
    "id": "ba0480d6-259b-4e54-854d-cef151b60dd9"
   },
   "source": [
    "#### build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303d567-91e2-4632-9a36-325f662ce3e9",
   "metadata": {
    "id": "6303d567-91e2-4632-9a36-325f662ce3e9"
   },
   "outputs": [],
   "source": [
    "def build_model(A, no_var): # A = A_ex or A_nex; no_var = 10\n",
    "\n",
    "    X_train = A[:, :no_var] # column 0 ~ 9 in A are P\n",
    "    Y_train = A[:, no_var:] # column 10 ~  in A are F_nex or F_exp respectively from A_ex or A_nex\n",
    "\n",
    "\n",
    "    if A.shape[0] > 500: # if A's number of rows > 500, perform a downsampling step\n",
    "        X_temp = np.array([])\n",
    "        Y_temp = np.array([])\n",
    "\n",
    "        kmeans = KMeans(n_clusters=500,n_init=1) # use kmeans to randomly select 500 data points on input data X_train\n",
    "        kmeans.fit(X_train) \n",
    "\n",
    "\n",
    "        idx = kmeans.fit_predict(X_train)\n",
    "        # print(\"idx from fit predict\")\n",
    "        # print(idx)\n",
    "\n",
    "        missing = np.setdiff1d(np.arange(0,500),idx)\n",
    "        # print(\"missing idx2\",missing2)\n",
    "\n",
    "\n",
    "        for n in range(500): # n = 0 to 499\n",
    "            t = np.where(idx == n) # t stores the indices where idx equals to n --> indices of the 500 data points assigned to cluster n by kmeans\n",
    "            pos = np.random.randint(len(t)) # pos is an integer randomly selected in the range 0 to len(t)-1 --> a random position what will be used to select a data point from the cluster assigned to n\n",
    "            ind = t[pos] # get the element at the position 'pos' from 't' array --> index of a specific data point from the cluster assigned to n\n",
    "            if len(X_temp) == 0:\n",
    "                X_temp = X_train[ind, :]\n",
    "            else:\n",
    "                X_temp = np.vstack((X_temp, X_train[ind, :]))\n",
    "            if len(Y_temp) == 0:\n",
    "                Y_temp= Y_train[ind, :]\n",
    "            else:\n",
    "                Y_temp = np.vstack((Y_temp, Y_train[ind, :]))\n",
    "        X_train = X_temp\n",
    "        Y_train = Y_temp\n",
    "\n",
    "    # is below the right way of translation from matlab?\n",
    "    kernel = RBF(length_scale_bounds=(1e-99, 1e+99))\n",
    "    model = GaussianProcessRegressor(kernel=kernel, alpha=1e-5, normalize_y=True,optimizer=None, n_restarts_optimizer=2000) #fmin_l_bfgs_b\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c745fdf-4970-43af-93fe-0930c464e4ef",
   "metadata": {
    "id": "9c745fdf-4970-43af-93fe-0930c464e4ef"
   },
   "source": [
    "#### P_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed162d34-f6f8-424d-a6da-828cfaf1d240",
   "metadata": {
    "id": "ed162d34-f6f8-424d-a6da-828cfaf1d240"
   },
   "outputs": [],
   "source": [
    "def P_sort(FunctionValue, operation): # for selecting the nondominated solutions\n",
    "\n",
    "    # Non-dominated sorting\n",
    "    # Input: FunctionValue - Population to be sorted (objective space)\n",
    "    #        operation - Specify sorting only the first front, sorting the first half individuals,\n",
    "    #                    or sorting all individuals. Default is to sort all individuals.\n",
    "    # Output: FrontValue - \"Front number\" where each individual belongs. Front number of unsorted individuals is infinity.\n",
    "    #         MaxFront - Maximum front number after sorting\n",
    "\n",
    "    if operation is None:\n",
    "        kinds = 1\n",
    "    elif operation == 'half':\n",
    "        kinds = 2\n",
    "    elif operation == 'first':\n",
    "        kinds = 3\n",
    "    else:\n",
    "        kinds = 1\n",
    "\n",
    "    N, M = FunctionValue.shape\n",
    "    MaxFront = 0\n",
    "\n",
    "    Sorted = np.zeros(N, dtype=bool)\n",
    "    rank = np.lexsort((FunctionValue[:, 1], FunctionValue[:, 0])) # sort 2nd key first, and then 1st key\n",
    "    FunctionValue_sorted = FunctionValue[rank]\n",
    "\n",
    "    FrontValue = np.full(N, np.inf)\n",
    "\n",
    "\n",
    "    while ((kinds == 1 and np.sum(Sorted) < N) or\n",
    "           (kinds == 2 and np.sum(Sorted) < N // 2) or\n",
    "           (kinds == 3 and MaxFront < 1)): # kinds == 3 and MaxFront < 1 is executed\n",
    "\n",
    "        MaxFront += 1\n",
    "        ThisFront = np.zeros(N, dtype=bool) # added\n",
    "\n",
    "        for i in range(N): # N = 152\n",
    "            if not Sorted[i]: # check if Sorted[i] is logically false, if it is flase set x = 0; if not false, skip\n",
    "                x = 0 # the one not sorted is set to x = 0 --> used as a flag to determine whether it is a dominated or nondominated soltuion\n",
    "                for j in range(N): # i = 0 ~ (N-1)\n",
    "                    if ThisFront[j]: # if ThisFront[j] is false, set x = 2; if not false, skip\n",
    "\n",
    "                        x = 2 # the one not True is set to x = 2\n",
    "                        for j2 in range(1, M): # since M = 2, so this for-loop only execute once --> j2 =2\n",
    "                            if FunctionValue_sorted[i, j2] < FunctionValue_sorted[j, j2]:\n",
    "                                x = 0\n",
    "                                break\n",
    "                        if x == 2:\n",
    "                            break\n",
    "                if x != 2:\n",
    "                    ThisFront[i] = True\n",
    "                    Sorted[i] = True\n",
    "\n",
    "\n",
    "        FrontValue[rank[ThisFront]] = MaxFront # matlab只有寫這樣\n",
    "\n",
    "    return FrontValue#, MaxFront"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d2241-e1e9-4338-b9c6-2a83bbd1a1f9",
   "metadata": {
    "id": "ae8d2241-e1e9-4338-b9c6-2a83bbd1a1f9"
   },
   "source": [
    "#### f_mating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2c703-c7da-444a-afe5-6e2fd8fd6c56",
   "metadata": {
    "id": "69a2c703-c7da-444a-afe5-6e2fd8fd6c56"
   },
   "outputs": [],
   "source": [
    "# evolutionary algorithm\n",
    "def F_mating(Population, pop_size):\n",
    "\n",
    "    D = Population.shape[1] # D = the dimension (# of columns) of Population --> 應為2\n",
    "    N = pop_size # N = desired(max) size of mating pool = 10 (defined in call_GA)\n",
    "\n",
    "    if Population.shape[0] < pop_size: # if the # of rows in Population < N(=10) 就用population的# of rows重新定義MatingPool的size\n",
    "        N = pop_size # determine N = the desired size of mating pool = # of rows of Population\n",
    "        D = Population.shape[1] # determine D = the dimensions(# of columns) of Population\n",
    "    MatingPool = np.zeros((N, D)) #重新定義的MaitingPool的size\n",
    "\n",
    "    for i in range(pop_size): # iterate N times to fill the MatingPool\n",
    "        RandList = np.random.permutation(Population.shape[0]) # generate a random permutation list 根據 Population 的 N 產出\"隨機順序\"的array (RandList是有 0 to N-1 的隨機排序array)\n",
    "        MatingPool[i, :] = Population[RandList[0], :] \n",
    "\n",
    "    if N % 2 == 1:\n",
    "        MatingPool = np.vstack((MatingPool, MatingPool[0, :])) # if N is odd, duplicate the first row of MatingPool and append to the last row to ensure even number of individuals in the pool\n",
    "\n",
    "\n",
    "    return MatingPool # MatingPool contains selected individuals from Population for offspring generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923bdd9-ed93-4dae-b22d-5c0b4a066a71",
   "metadata": {
    "id": "b923bdd9-ed93-4dae-b22d-5c0b4a066a71"
   },
   "source": [
    "#### f_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761a1bb-b083-43f6-a4f3-16b15abee079",
   "metadata": {
    "id": "5761a1bb-b083-43f6-a4f3-16b15abee079"
   },
   "outputs": [],
   "source": [
    "def F_select(FunctionValue, V, theta, refV):\n",
    "\n",
    "\n",
    "    my_ref = [] # to store reference vectors corresponding to selected individuals\n",
    "    Empty_ref = [] # to store reference vectors corresponding to empty sets\n",
    "    APD = [] # to store average perpendicular distance values (APD)\n",
    "    N, M = FunctionValue.shape # FunctionValue's shape (N,M)\n",
    "    VN = V.shape[0] # number of row of reference vectors V\n",
    "\n",
    "    Zmin = np.min(FunctionValue, axis=0) # min of each 'column' in FunctionValue --> will have M values\n",
    "    Zmax = np.max(FunctionValue, axis=0) # max of each 'column' in FuncitonValue --> will have M values\n",
    "\n",
    "    # FunctionValue = (FunctionValue - np.tile(Zmin, (FunctionValue.shape[0], 1))) #a new FunctionValue with each value subtracting the Zmin of its corresponding column\n",
    "    FunctionValue = (FunctionValue - Zmin)\n",
    "\n",
    "    uFunctionValue = np.empty_like(FunctionValue)\n",
    "\n",
    "    has_nan = np.isnan(FunctionValue).any()\n",
    "    has_inf = np.isinf(FunctionValue).any()\n",
    "    if has_nan:\n",
    "        print(\"FunctionValue array contains NaN values.\")\n",
    "    if has_inf:\n",
    "        print(\"FunctionValue array contains infinity values.\")\n",
    "\n",
    "    # 0722 修改\n",
    "    for i in range(N):\n",
    "      row_norm = np.linalg.norm(FunctionValue[i, :])\n",
    "      if row_norm < 1e-8:  # Set a small threshold to avoid division by zero\n",
    "          uFunctionValue[i, :] = np.zeros_like(FunctionValue[i, :])\n",
    "      else:\n",
    "          uFunctionValue[i, :] = FunctionValue[i, :] / row_norm\n",
    "\n",
    "\n",
    "\n",
    "    cosine = np.dot(uFunctionValue, V.T) # calculate consine similarity between normalised FunctionValue and reference vectors\n",
    "\n",
    "    acosine = np.arccos(cosine) \n",
    "\n",
    "\n",
    "    maxc = np.max(cosine, axis = 1)\n",
    "    maxcidx = np.argmax(cosine, axis = 1) # get the maximum reference vector's index with the max consine similarity for each individual(row)\n",
    "    # print(\"maxcdix\")\n",
    "    # print(maxcidx) \n",
    "\n",
    "    class_dict = {k: [] for k in range(VN)}\n",
    "    for i in range(N):\n",
    "        class_dict[maxcidx[i]].append(i) # iterate over each individual and append its index to the corresponding class list based on maxcidx\n",
    "\n",
    "\n",
    "    # below array all modified to list\n",
    "    Selection = [] # indices of selected individuals\n",
    "    FitnessValue = [] # fitness values of selected individuals --> what is this for???? it is not used in this function\n",
    "    EmptySet = [] # indices of empty sets\n",
    "    NonEmptySet = [] # indices of non-empty sets\n",
    "\n",
    "    # selection process for each class (maximum reference vector's index)\n",
    "    for k in range(VN): # iterate over each class\n",
    "        if len(class_dict[k]) != 0: # check if the class is non-empty, which has individuals assigned to. if it's non-empty, perform selection process\n",
    "            sub = class_dict[k]\n",
    "            subFunctionValue = FunctionValue[sub, :]\n",
    "            subacosine = acosine[sub, k]\n",
    "            D1 = np.sqrt(np.sum(subFunctionValue**2, axis=1))\n",
    "            subacosine = subacosine / refV[k]\n",
    "\n",
    "            # if np.isclose(np.sum(V[k, :]), 1.0):\n",
    "            if np.sum(V[k, :]) >= 0.9999999 and np.sum(V[k, :]) <= 1.0005: # & seems to have problem: ufunc \"bitwise_and\" not supported for the input types\n",
    "                D = D1 * (1 + 10**200 * theta * subacosine) # 1e200 not equal to 10**200 though they are same values, but 1e200 is floatin-point, 10**200 is integer\n",
    "            else:\n",
    "                D = D1 * (1 + theta * subacosine)\n",
    "\n",
    "            mindidx = np.argmin(D)\n",
    "            mind = D[mindidx] # 0720 added\n",
    "\n",
    "            Selection.append(sub[mindidx])\n",
    "            APD.append(mind)\n",
    "            my_ref.append(V[k,:])\n",
    "        else:\n",
    "            EmptySet.append(k)\n",
    "            Empty_ref.append(V[k,:])\n",
    "\n",
    "\n",
    "    return Selection, APD, my_ref, Empty_ref\n",
    "\n",
    "    # Selection = selected individuals\n",
    "    # APD = selected individuals' correpsonding APD\n",
    "    # my_ref = selected individuals' corresponding reference vectors\n",
    "    # F_select provides the most promising individuals selected, there correspdoning APD and reference vectors for further use in EA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158ab4f-1f45-4245-9b85-0772201e861f",
   "metadata": {
    "id": "c158ab4f-1f45-4245-9b85-0772201e861f"
   },
   "source": [
    "#### f_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b1e1f-3088-4ca1-8c84-b4d2ceb9043f",
   "metadata": {
    "id": "ee1b1e1f-3088-4ca1-8c84-b4d2ceb9043f"
   },
   "outputs": [],
   "source": [
    "def F_weight(p1, p2, M): # (99,0,2) from ref_vectors\n",
    "\n",
    "    N, W = T_weight(int(p1), M) # should take off int()? --> W is from T_weight and is a 100x2 matrix\n",
    "    if p2 > 0:\n",
    "        N2, W2 = T_weight(int(p2), M)\n",
    "        N = N + N2\n",
    "        W = np.vstack((W, W2 * 0.5 + (1 - 0.5) / M))\n",
    "    return N, W\n",
    "\n",
    "    # N = updated # of weight vectors\n",
    "    # W = combined weight matrix -->for balancing the weight importance of each objective in the optimisation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otjKnTvycjjY",
   "metadata": {
    "id": "otjKnTvycjjY"
   },
   "source": [
    "#### t_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoxkdakUceyn",
   "metadata": {
    "id": "LoxkdakUceyn"
   },
   "outputs": [],
   "source": [
    "def T_weight(H, M): # (99,2)\n",
    "\n",
    "    # N = int(comb(H + M - 1, M - 1, exact=True)) # N = number of weight vectors\n",
    "    N = math.comb(H+M-1, M-1) # 這樣 N 為 100 if p1 = H = 99\n",
    "    Temp = np.zeros((N, M - 1))\n",
    "    for i in range(N):\n",
    "        Temp[i, :] = np.arange(1, M) - 1 + i\n",
    "    W = np.zeros((N, M)) # initialise empty weight matrix with the shape (N,M)\n",
    "    W[:, 0] = Temp[:, 0] - 0\n",
    "    for i in range(1, M-1):\n",
    "        W[:, i] = Temp[:, i] - Temp[:, i - 1] # calculate the weight values for each weight vector in W\n",
    "    W[:, -1] = H - Temp[:, -1]\n",
    "    W = W / H # normalise W --> checked, W has the same output as in matlab\n",
    "\n",
    "    return N, W\n",
    "    # N = number of weight vectors; W = weight matrix with the shape of (N,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1221307-3dc0-489c-8900-e848b858bf93",
   "metadata": {
    "id": "d1221307-3dc0-489c-8900-e848b858bf93"
   },
   "source": [
    "#### p_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe73ae5-fc99-4487-beb7-a52e8e3598c8",
   "metadata": {
    "id": "cfe73ae5-fc99-4487-beb7-a52e8e3598c8"
   },
   "outputs": [],
   "source": [
    "def P_generator(MatingPool, Boundary, Coding, MaxOffspring, ProC, ProM):\n",
    "\n",
    "    N, D = MatingPool.shape # D is dimension in decision space\n",
    "\n",
    "    MaxOffspring = N\n",
    "\n",
    "\n",
    "    if Coding == 'Real':\n",
    "        # simulated binary crossover (SBX)\n",
    "        ProC = cross_prob # crossover probability\n",
    "        DisC = 20 # crossover's 20 distribution index\n",
    "        # polynomial mutation\n",
    "        ProM = mut_prob # mutation probability\n",
    "        DisM = 20 # mutation's 20 distribution index\n",
    "\n",
    "\n",
    "\n",
    "        Offspring = np.zeros((N, D)) # 0715新增\n",
    "\n",
    "        # performing SBX to generate offsprings from MatingPool(obtained from populatoin or so-called parents)\n",
    "        for i in range(0, N, 2):\n",
    "            beta = np.zeros(D)\n",
    "            miu = np.random.rand(D)\n",
    "            beta[miu <= 0.5] = (2 * miu[miu <= 0.5]) ** (1 / (DisC + 1))\n",
    "            beta[miu > 0.5] = (2 - 2 * miu[miu > 0.5]) ** (-1 / (DisC + 1))\n",
    "            beta *= (-1) ** np.random.randint(0, 2, D) # 這是要beta乘以相對應的(-1的隨機0,1,2次方)\n",
    "            beta[np.random.rand(D) > ProC] = 1\n",
    "\n",
    "            # generate offspring based on above formula\n",
    "            Offspring[i, :] = [(MatingPool[i, :] + MatingPool[i + 1, :]) / 2] + beta * (MatingPool[i, :] - MatingPool[i + 1, :]) / 2\n",
    "            Offspring[i + 1, :] = (MatingPool[i, :] + MatingPool[i + 1, :]) / 2 - beta * (MatingPool[i, :] - MatingPool[i + 1, :]) / 2\n",
    "\n",
    "        # trunacte Offspring, only the first N rows are selected to be Offspring --> ensure size same as MaxOffspring\n",
    "        Offspring = Offspring[:MaxOffspring, :]\n",
    "\n",
    "        # handle boundary constraint (max and min values) for each dimension\n",
    "        if MaxOffspring == 1:\n",
    "            MaxValue = Boundary[0, :]\n",
    "            MinValue = Boundary[1, :]\n",
    "        else:\n",
    "            MaxValue = np.tile(Boundary[0, :], (MaxOffspring, 1))\n",
    "            MinValue = np.tile(Boundary[1, :], (MaxOffspring, 1))\n",
    "\n",
    "        # performing polynomial mutation to diversify the generated Offspring\n",
    "        k = np.random.rand(MaxOffspring, D)\n",
    "        miu = np.random.rand(MaxOffspring, D)\n",
    "        Temp = np.logical_and(k <= ProM, miu < 0.5)\n",
    "        Offspring[Temp] = Offspring[Temp] + (MaxValue[Temp] - MinValue[Temp]) * ((2 * miu[Temp] + (1 - 2 * miu[Temp]) \\\n",
    "            * (1 - (Offspring[Temp] - MinValue[Temp]) / (MaxValue[Temp] - MinValue[Temp])) ** (DisM + 1)) ** (1 / (DisM + 1)) - 1)\n",
    "        Temp = np.logical_and(k <= ProM, miu >= 0.5)\n",
    "        Offspring[Temp] = Offspring[Temp] + (MaxValue[Temp] - MinValue[Temp]) * (1 - (2 * (1 - miu[Temp]) + 2 * (miu[Temp] - 0.5) \\\n",
    "            * (1 - (MaxValue[Temp] - Offspring[Temp]) / (MaxValue[Temp] - MinValue[Temp])) ** (DisM + 1)) ** (1 / (DisM + 1)))\n",
    "\n",
    "        Offspring[Offspring > MaxValue] = MaxValue[Offspring > MaxValue]\n",
    "        Offspring[Offspring < MinValue] = MinValue[Offspring < MinValue]\n",
    "\n",
    "    return Offspring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb2d15-7583-4d37-94c9-960798fa13c4",
   "metadata": {
    "id": "f6bb2d15-7583-4d37-94c9-960798fa13c4"
   },
   "source": [
    "#### evolve k rvea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf13c94-9640-4922-9b7f-db23aa25f5fd",
   "metadata": {
    "id": "6bf13c94-9640-4922-9b7f-db23aa25f5fd"
   },
   "outputs": [],
   "source": [
    "# 此function是個使用Kriging model的EA\n",
    "# 為了找samples來更新Kriding model\n",
    "# update_metamodel得到的是my_pop跟empty_new_ref就是更新後的(evolve_k_rvea的)off跟empty_ref_old\n",
    "# 最後產生off放到run_k_rvea的pop作為下一個iteration的P(去重新被f1,f2評估)\n",
    "def evolve_K_RVEA(model_ex, model_nex, Archive, Boundary, Empty_ref_old, id_ex, id_nex):\n",
    "\n",
    "    no_var = Boundary.shape[1] # 10\n",
    "    Population = Archive[:, :no_var]\n",
    "    FunctionValue = Archive[:, no_var:]\n",
    "\n",
    "    maxFE = 1000\n",
    "    Coding = 'Real'\n",
    "\n",
    "    Vs = ref_vectors(2)  # Generation of reference vectors\n",
    "    V = Vs\n",
    "\n",
    "    N = V.shape[0]\n",
    "    M = FunctionValue.shape[1]\n",
    "    Fix_V = Vs\n",
    "    up_var = M\n",
    "    alpha = 0.5\n",
    "    cosineVV = np.dot(V, V.T)\n",
    "    scosineVV = np.sort(cosineVV, axis=1)[:, ::-1]\n",
    "    acosVV = np.arccos(scosineVV[:, 1])\n",
    "    refV = acosVV\n",
    "\n",
    "    FE = 0\n",
    "    w = 0\n",
    "\n",
    "    ERR = np.zeros((Population.shape[0],1)) # ERR's shape (10x1)\n",
    "    # print(\"ERR\")\n",
    "    # print(ERR)\n",
    "    metagen = round(maxFE/N) + 200 \n",
    "\n",
    "    # K-RVEA應用kriging model得到uncertainty values (ER)跟APD 並且看是要用diversity或convergence去選下一個iteration的P\n",
    "    while FE < maxFE:\n",
    "        # SBX and polynomial mutation\n",
    "        MatingPool = F_mating(Population, N)\n",
    "\n",
    "        Offspring = P_generator(MatingPool, Boundary, Coding, N, cross_prob, mut_prob)\n",
    "\n",
    "\n",
    "        ER = np.zeros((Offspring.shape[0], M))\n",
    "        # print(\"offsrping shape\", Offspring.shape) #(14,2) so ER will be a shape of (14,2)\n",
    "        Fitness = np.zeros((Offspring.shape[0], M))\n",
    "\n",
    "        result_ex, std_ex = model_ex.predict(Offspring, return_std=True) # 用Kriging model的結果來得到unvertainty values (ER)\n",
    "        Fitness[:, id_ex-1] = result_ex\n",
    "        ER[:, id_ex-1] = std_ex**2\n",
    "        # ER[:, id_ex-1] = std_ex\n",
    "\n",
    "        result_nex, std_nex = model_nex.predict(Offspring, return_std=True) # 用Kriging model的結果來得到uncertainty values (ER)\n",
    "        Fitness[:, id_nex-1] = result_nex\n",
    "        ER[:, id_nex-1] = std_nex**2\n",
    "        # ER[:, id_nex-1] = std_nex**2\n",
    "\n",
    "        FE += Fitness.shape[0]\n",
    "\n",
    "        ER = np.mean(ER, axis=1).reshape(-1,1) # mean of each row\n",
    "\n",
    "        Population = np.vstack((Population, Offspring))\n",
    "        FunctionValue = np.vstack((FunctionValue, Fitness))\n",
    "\n",
    "        # 原本ERR是一個(10x1)的matrix但裡面都是0 這樣vstack進去會先是10個0再來是ER的值 感覺怪怪的? 不過在matlab寫這code產出也是這樣\n",
    "        ERR = np.vstack((ERR, ER)) #ERR(10x1) ER(14x1)\n",
    "\n",
    "        if M == 2:\n",
    "            theta = (FE / maxFE) ** alpha\n",
    "\n",
    " \n",
    "        else:\n",
    "           theta = M * (w / metagen) ** alpha\n",
    "\n",
    "        # f_select選出individuals, 其對應的APD, active reference vectors(有被individuals分配到的reference vectors)跟empty_ref(沒被individuals分配到的reference vectors)\n",
    "        Selection, APD, my_ref, Empty_ref = F_select(FunctionValue, V, theta, refV)\n",
    "        Population = Population[Selection, :]\n",
    "        FunctionValue = FunctionValue[Selection, :]\n",
    "        ERR = ERR[Selection, :]\n",
    "\n",
    "\n",
    "        if (FE % np.ceil(maxFE * 0.3) == 0 and FE > 0):\n",
    "            Zmin = np.min(FunctionValue, axis=0)\n",
    "            Zmax = np.max(FunctionValue, axis=0)\n",
    "\n",
    "            V = Vs\n",
    "            V = V * np.tile((Zmax - Zmin) * 1.0, (N, 1))\n",
    "            tV = V\n",
    "\n",
    "            for i in range(N):\n",
    "                V[i, :] = V[i, :] / np.linalg.norm(V[i, :])\n",
    "\n",
    "            cosineVV = np.dot(V, V.T)\n",
    "            scosineVV = np.sort(cosineVV, axis=1)[:, ::-1]\n",
    "            acosVV = np.arccos(scosineVV[:, 1])\n",
    "            refV = acosVV\n",
    "        w += 1\n",
    "\n",
    "    info_update = {'c':[FunctionValue, V, theta, Fix_V, Empty_ref_old, refV, up_var, N, Population, ERR, Boundary]} \n",
    "\n",
    "    off, Empty_ref_old = update_metamodel(info_update) # use update_metamodel to update metamodel with the info_update dictionary that results in off (Offspring) and Empty_ref_old (updated reference vectors)\n",
    "\n",
    "\n",
    "    off = np.unique(off, axis=0) # remove duplicated rows of off (off means the population selected for re-evaluation with f_ex to update Kriging model)\n",
    "    current_pop = Population\n",
    "\n",
    "    P_archive = Archive[:, :no_var]\n",
    "\n",
    "    Lia = np.all(np.isin(off, P_archive), axis=1) # compare Archive with off (Offspring from updated metamodel) to identify \"newly found nondominated solutions\"\n",
    "    r_unique = np.where(Lia == 0)[0]\n",
    "    off = off[r_unique, :]\n",
    "    tt = 1\n",
    "    while len(off) == 0: # to ensure at least one unique offspring is found. if off is empty, go into this loop\n",
    "        rt = np.random.permutation(current_pop.shape[0])\n",
    "        off = current_pop[rt[0:1], :] # select random solution from current_pop\n",
    "        Lia = np.all(np.isin(off, P_archive), axis=1)\n",
    "        r_unique = np.where(Lia == 0)[0] # check again dupliation of off\n",
    "        off = off[r_unique, :]\n",
    "        tt += 1\n",
    "        if tt > 1000: # check up to 1000 times until a unique solution is found\n",
    "            break\n",
    "\n",
    "    return off, Empty_ref_old\n",
    "\n",
    "    # off = unique offspring solutions; Empty_ref_old = updated reference vectors\n",
    "    # this EA function includes mating, mutation, evaluation, selection, and metamodel updating\n",
    "    # with the goal of discovering and improving non-dominated solutions in multi-objective optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae33c4-9ebd-4939-8cb1-dd1c103a98b0",
   "metadata": {
    "id": "86ae33c4-9ebd-4939-8cb1-dd1c103a98b0"
   },
   "source": [
    "#### run_k_rvea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeae37-2855-4fe6-90ab-615f614e9f7c",
   "metadata": {
    "id": "bedeae37-2855-4fe6-90ab-615f614e9f7c"
   },
   "outputs": [],
   "source": [
    "# run K-RVEA\n",
    "def run_K_RVEA(model_ex, model_nex, Boundary, A, id_ex, id_nex, empty_ref):\n",
    "\n",
    "    pop, empty_ref = evolve_K_RVEA(model_ex, model_nex, A, Boundary, empty_ref, id_ex, id_nex)\n",
    "\n",
    "    return pop, empty_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2d4d2-4aa1-43ae-8f86-9962878459df",
   "metadata": {
    "id": "f8d2d4d2-4aa1-43ae-8f86-9962878459df"
   },
   "source": [
    "#### ref_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813de887-8909-4024-b505-3730565fadf3",
   "metadata": {
    "id": "813de887-8909-4024-b505-3730565fadf3"
   },
   "outputs": [],
   "source": [
    "# there will be |M| p when having M objectives, so when M=2, there are only p1 and p2, but this will have to change in the future\n",
    "def ref_vectors(M): # p1,p2是k-rvea論文裡面地131頁那個用simplex-lattice design弄出來的 \n",
    "\n",
    "    p1 = np.array([99, 13, 7, 5, 4, 3, 3, 2, 3])\n",
    "    p2 = np.array([0, 0, 0, 0, 1, 2, 2, 2, 2]) \n",
    "    p1 = p1[M - 2] \n",
    "    p2 = p2[M - 2]\n",
    "    if M > 10: # suggesting that the reference vectors do not support for more than 10 objectives\n",
    "        raise ValueError('Define the number of ref vectors parameters for objectives > 10')\n",
    "\n",
    "    _, Vs = F_weight(p1, p2, M) # only get Vs from W(combined weight matrix) in F_weight --> W is a 100x3 matrix\n",
    "    # Vs[Vs ==0] = 0.000000001\n",
    "\n",
    "    # getting reference vectors v1 by p1/(L2-norm of p1) and v2 by p2/(L2-norm of p2)\n",
    "    for i in range(Vs.shape[0]): # each row of the generated V array is a reference vector\n",
    "        Vs[i, :] = Vs[i, :] / np.linalg.norm(Vs[i, :]) # each row of the generated V array is normalised\n",
    "\n",
    "    return Vs # normalised Vs array for given number of objective (normalisation is to ensure reference vecotrs have unit length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8c8ed-2d0f-41ad-a869-09c0d807547e",
   "metadata": {
    "id": "60a8c8ed-2d0f-41ad-a869-09c0d807547e"
   },
   "source": [
    "#### tournamentSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dfdd70-9434-4a69-a7f6-48011e42ac0b",
   "metadata": {
    "id": "c6dfdd70-9434-4a69-a7f6-48011e42ac0b"
   },
   "outputs": [],
   "source": [
    "def tournamentSelection(tournSize, fitness):\n",
    "\n",
    "    n = len(fitness) # n = length(#) of fitness values for each individual in the population (so n = the number of rows of population)\n",
    "    winners = []\n",
    "\n",
    "    # Make sure that the population size is divisible by the tournament size\n",
    "    if n % tournSize != 0: #tournSize = # of competitors\n",
    "        raise ValueError('Population size has to be divisible by the tournament size')\n",
    "\n",
    "    tourn_count = 1\n",
    "    # Repeat the tournament selection process \"tournSize\" times\n",
    "    for i in range(tournSize):\n",
    "        shuffleOrder = np.random.permutation(n) # create a random set of competitors from 0 to (n-1)\n",
    "        competitors = shuffleOrder.reshape((n // tournSize, tournSize)) # shape(tournSize,n/tournSize) with numbers 0 to (n-1)\n",
    "\n",
    "        fitness_comp = fitness[competitors]  # fitness[competitors] will retrieve the value from fitness based on competitor's index (e.g. fitness[0] is 0.2 then the new fitness will be formed based on the fitness[comp's index])\n",
    "\n",
    "        # The winner is the competitor with the best fitness\n",
    "        # winner, the competitors with max fitness value, assigned to winFit\n",
    "        winFit = np.max(fitness[competitors], axis=1) # create new fitness array and compare the fitness value of each row. retrieve the biggest number of each row to winFit array --> winFit = a shape of 10x1 with the biggest fitness value of each row\n",
    "\n",
    "\n",
    "        winID = np.argmax(fitness[competitors], axis=1) # based on the new fitness array, assign new index for each row (so only 0 and 1 for each row) --> winID = a shape of 10x1 with the index of the biggest value of each row in the new fitness array (fitness[competitors])\n",
    "\n",
    "        idMap = np.arange(tournSize) * (n // tournSize)\n",
    "\n",
    "        idMapwinID = np.squeeze(np.array([idMap[w] for w in winID]))\n",
    "\n",
    "        idMap1 = idMapwinID + np.arange(competitors.shape[0])\n",
    "\n",
    "        if len(winners) == 0:\n",
    "          winners = np.hstack(competitors.T.reshape(1,-1)[0,idMap1])\n",
    "        else:\n",
    "          winners = np.vstack((winners, competitors.T.reshape(1,-1)[0,idMap1]))\n",
    "\n",
    "        tourn_count+=1\n",
    "    # print(\"winnders in tournament\",winners)\n",
    "\n",
    "    return winners # a list of final winners (individuals with max fitness values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53901f2a-93c7-4b68-9200-5b499d3946d1",
   "metadata": {
    "id": "53901f2a-93c7-4b68-9200-5b499d3946d1"
   },
   "source": [
    "#### update metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3uwzNtfFZ3QC",
   "metadata": {
    "id": "3uwzNtfFZ3QC"
   },
   "outputs": [],
   "source": [
    "def update_metamodel(info):\n",
    "\n",
    "    FunctionValue = info['c'][0]\n",
    "    V = info['c'][1]\n",
    "    theta = info['c'][2]\n",
    "    Fix_V = info['c'][3]\n",
    "    Empty_ref_old = info['c'][4]\n",
    "    refV = info['c'][5]\n",
    "    up_var = info['c'][6]\n",
    "    N = info['c'][7]\n",
    "    Population = info['c'][8]\n",
    "    ERR = info['c'][9]\n",
    "\n",
    "    M = FunctionValue.shape[1]\n",
    "\n",
    "    if not isinstance(Empty_ref_old, (np.ndarray, list, tuple)):\n",
    "        Empty_ref_old = np.array([])  # Initialize as an empty NumPy array\n",
    "    elif not hasattr(Empty_ref_old, 'shape'):\n",
    "        raise ValueError(\"Empty_ref_old should have the attribute 'shape'.\")\n",
    "\n",
    "    _, Empty_ref_new, *_ = reference(FunctionValue, Fix_V, refV, ERR, Population, theta)\n",
    "    delta = len(Empty_ref_new) - len(Empty_ref_old) \n",
    "\n",
    "    # find active reference vectors\n",
    "    _, _, Non_empty_ref, APD_class, ER_class, Population_class = reference(FunctionValue, V, refV, ERR, Population, theta)\n",
    "\n",
    "    if len(Non_empty_ref) < up_var: # non_empty_ref row # = 10, up_var = 2 \n",
    "        cluster_size = len(Non_empty_ref)\n",
    "    else:\n",
    "        cluster_size = up_var # cluster size = 2\n",
    "\n",
    "\n",
    "    if Population.shape[0] < 2:\n",
    "        my_pop = Population\n",
    "\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=cluster_size, init='random')\n",
    "        kmeans.fit(Non_empty_ref)\n",
    "        idx = kmeans.labels_\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        # Check if any cluster is empty\n",
    "        empty_clusters = np.where(np.bincount(idx) == 0)[0]\n",
    "\n",
    "        # If there are empty clusters, create a new cluster with a single point farthest from its centroid\n",
    "        if len(empty_clusters) > 0:\n",
    "            max_distance = -1\n",
    "            farthest_point = None\n",
    "\n",
    "            for cluster in empty_clusters:\n",
    "                cluster_points = Non_empty_ref[idx == cluster]\n",
    "                cluster_centroid = centroids[cluster]\n",
    "                distances = np.linalg.norm(cluster_points - cluster_centroid, axis=1)\n",
    "                max_dist_idx = np.argmax(distances)\n",
    "                if distances[max_dist_idx] > max_distance:\n",
    "                    max_distance = distances[max_dist_idx]\n",
    "                    farthest_point = cluster_points[max_dist_idx]\n",
    "\n",
    "            # Create a new cluster with the farthest point\n",
    "            new_cluster = np.expand_dims(farthest_point, axis=0)\n",
    "            centroids[empty_clusters[0]] = farthest_point\n",
    "            idx[idx == empty_clusters[0]] = cluster_size  # Assign the new cluster label\n",
    "        my_pop = np.array([])\n",
    "\n",
    "        for i in range(cluster_size):\n",
    "          rr = np.where(idx == i)[0] #[0] should be deleted right?\n",
    "\n",
    "          # 看是要依據convergence還是diversity作為標準去選individuals --> if: 用diversity(最大uncertainty) else:用convergence(最小APD)\n",
    "          if delta < 0.05 * N: # if delta < 0.05N, it means that it's an empty reference vector(the vector not being assigned any individuals) --> 'convergence' as priority criterion --> choose the one with smallest APD\n",
    "              need = []\n",
    "              for j in range(len(rr)):\n",
    "                  APD_new = APD_class[rr[j]]\n",
    "                  APD_new, index = np.min(APD_new), np.argmin(APD_new)\n",
    "                  need.append([rr[j], index, APD_new])\n",
    "              index_final = np.argmin(np.array(need)[:,-1]) #改\n",
    "              need = need[index_final]\n",
    "              sub_class = need[0]\n",
    "              sub_class_entry = need[1]\n",
    "              if np.array(my_pop).shape[0] == 0:\n",
    "                my_pop = Population_class[sub_class][sub_class_entry:]\n",
    "              else:\n",
    "                my_pop = np.vstack((my_pop, Population_class[sub_class][sub_class_entry:]))\n",
    "\n",
    "          else: # delta >= 0.05N means it's an active reference vector --> consider 'diversity' criterion --> choose the one with largest uncertaintu information obtained from kriging model\n",
    "              need = []\n",
    "              for j in range(len(rr)):\n",
    "                  ER_new = ER_class[rr[j]] # 0720 added c\n",
    "                  ER_new, index = np.max(ER_new), np.argmax(ER_new)\n",
    "                  need.append([rr[j], index, ER_new])\n",
    "\n",
    "              index_final = np.argmax(np.array(need)[:,-1]) # 改? got problem here\n",
    "              need = need[index_final]\n",
    "              sub_class = need[0]\n",
    "              sub_class_entry = need[1]\n",
    "              if np.array(my_pop).shape[0] == 0:\n",
    "                my_pop = Population_class[sub_class][sub_class_entry:]\n",
    "              else:\n",
    "                my_pop = np.vstack((my_pop, Population_class[sub_class][sub_class_entry:]))\n",
    "\n",
    "\n",
    "    return my_pop, Empty_ref_new\n",
    "\n",
    "    # my_pop is the population selected for re-evaluation with f_ex for the purpose of updating Kriging model\n",
    "    # 也就是作為下一個iteration用的P\n",
    "    # 這個my_pop會給到evolve_k_rvea的off再給到run_k_rvea裡的pop\n",
    "    # 最後再從algorithm的第6步把pop丟給P 作為下一iteration的P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2bd70-6fbf-4230-b18d-67798e968e5c",
   "metadata": {
    "id": "e3e2bd70-6fbf-4230-b18d-67798e968e5c"
   },
   "source": [
    "#### reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4f25f-17fb-4042-936a-7a8d896d2ae5",
   "metadata": {
    "id": "4eb4f25f-17fb-4042-936a-7a8d896d2ae5"
   },
   "outputs": [],
   "source": [
    "def reference(fitness, V, refV, ER, Population, theta):\n",
    "\n",
    "    N, M = fitness.shape # we get the fitness value by FunctionValue in update_metamodel function --> N,M = population shape\n",
    "    VN = V.shape[0]\n",
    "    Empty_ref = [] # empty reference vector\n",
    "    empty_rows = []\n",
    "    fill_ref = [] # active referece vector\n",
    "\n",
    "    Zmin = np.min(fitness, axis=0) # get min of each column in fitness\n",
    "    fitness = (fitness - Zmin) # normalised fitness\n",
    "\n",
    "    class_dict = {}\n",
    "\n",
    "    row_norm = np.linalg.norm(fitness, axis=1)  # Calculate the Euclidean norm of each row\n",
    "    mask = row_norm != 0  # Create a mask for non-zero norms\n",
    "    ufitness = np.zeros_like(fitness)  # Initialize ufitness array\n",
    "\n",
    "    ufitness[mask, :] = fitness[mask, :] / row_norm[mask, np.newaxis]  # Perform element-wise division for non-zero norms\n",
    "    ufitness[~mask, :] = np.nan  # Set rows with zero norms to NaN\n",
    "\n",
    "    cosine = ufitness @ V.T # calculate the consince similarity between normalised fitness values and reference vectors\n",
    "    cosine = np.clip(cosine, -1, 1) \n",
    "    acosine = np.arccos(cosine) # calculate arccosine of the cosine similarity values # 0722 runtime warning as in f_select!\n",
    "\n",
    "    maxc = np.max(cosine, axis=1) # 0719 added\n",
    "    maxcidx = np.argmax(cosine, axis=1) # get the index of maximum cosine similarity index\n",
    "\n",
    "\n",
    "    class_dict = {\"c\": [[] for _ in range(VN)]} # 0720 modified\n",
    "\n",
    "    for i in range(N):\n",
    "      class_dict['c'][maxcidx[i]].append(i)\n",
    "\n",
    "    jj = 0\n",
    "\n",
    "    Population_class = {}\n",
    "    APD_class = {}\n",
    "    ER_class = {}\n",
    "\n",
    "    for k in range(VN): # check each reference vector k\n",
    "        if len(class_dict[\"c\"][k]) == 0: # if the reference vector doesn't have any individuals in class_dict, it is a empty reference vector --> if it's an empty vector, use diversity as criterion and select the individuals with maximum uncertainty calculated from Kriging model\n",
    "            Empty_ref.append(V[k,:])\n",
    "            empty_rows.append(k)\n",
    "        else: # otherwise, it's a filled reference vector = active reference vector --> use convergence as criterion and select the individuals with minimum APD\n",
    "            sub = class_dict['c'][k] # the individual is retrieved from the class to 'sub'\n",
    "            # the corresponding values, populations, error rate... are assigned to the class-specific lists\n",
    "            subFunctionValue = fitness[sub, :]\n",
    "            Population_class[jj] = Population[sub,:] \n",
    "            subacosine = acosine[sub, k]\n",
    "            subacosine = subacosine / refV[k]\n",
    "            D1 = np.sqrt(np.sum(subFunctionValue ** 2, axis=1))\n",
    "            APD_class[jj] = D1* (1+(theta*subacosine)) \n",
    "            ER_class[jj] = ER[sub,:] \n",
    "            fill_ref.append(V[k,:])\n",
    "            jj += 1\n",
    "\n",
    "    return class_dict, Empty_ref, fill_ref, \\\n",
    "          APD_class, ER_class, Population_class \n",
    "    # these are the relevant information for updating metamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e142f-e535-4f97-b5ff-31ca5fbbe242",
   "metadata": {
    "id": "1e4e142f-e535-4f97-b5ff-31ca5fbbe242"
   },
   "source": [
    "### Part 2.2: Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e3b73-06d3-4f09-832c-6a6d48112252",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc9e3b73-06d3-4f09-832c-6a6d48112252",
    "outputId": "b42ab41b-0295-46ce-ab7d-1fe395497f2f"
   },
   "outputs": [],
   "source": [
    "# main algorithm\n",
    "maxrun = 21 # should be 21\n",
    "\n",
    "# to store results\n",
    "solutions_dict = {}\n",
    "median_pareto_front_dict = {}\n",
    "results_dict = {}\n",
    "results_dict_norm = {}\n",
    "\n",
    "max_values_all = {}\n",
    "min_values_all = {}\n",
    "\n",
    "hkrvea_at1 = []\n",
    "hkrvea_at2 = []\n",
    "hkrvea_hv = []\n",
    "\n",
    "reference_point = np.array([3,3]) # ref越小 hv越小\n",
    "\n",
    "max_of_each_lat = []\n",
    "min_of_each_lat = []\n",
    "\n",
    "max_nex_all = []\n",
    "min_nex_all = []\n",
    "max_ex_all = []\n",
    "min_ex_all = []\n",
    "\n",
    "hkrvea = []\n",
    "\n",
    "\n",
    "\n",
    "for lat in range(len(Latencies)):\n",
    "  latency = Latencies[lat]\n",
    "  Max_FE_ex = round(Max_FE_nex/latency) # maximum number of expensive evaluations for expensive objective functions\n",
    "\n",
    "\n",
    "  latency_dict = {}\n",
    "  median_PF_A_all = np.array([])\n",
    "\n",
    "\n",
    "  max_values_list = []\n",
    "  min_values_list = []\n",
    "\n",
    "  # hkrvea_hypervolumes = []\n",
    "\n",
    "  max_nex_list = []\n",
    "  min_nex_list = []\n",
    "  max_ex_list = []\n",
    "  min_ex_list = []\n",
    "  for p in range(len(Problems)): # only 2 problems as benchmark\n",
    "    Problem = Problems[p]\n",
    "\n",
    "    for run in range(maxrun):\n",
    "      # step 1-1: initialise itr_count = 0, initialize FEex = 0, FEnex = 0, A = φ and Anex = φ\n",
    "      itr_count = 0\n",
    "      FE_ex = 0\n",
    "      FE_nex = 0\n",
    "      empty_ref = 0\n",
    "      A = np.array([])\n",
    "      A_nex = np.array([])\n",
    "      A_ex = np.array([])\n",
    "\n",
    "\n",
    "\n",
    "      for j in range(len(no_obj)): #\n",
    "          M = no_obj[j]\n",
    "\n",
    "          # step 1-2: Generate no. of samples (p) by using \"latin hypercube\"\n",
    "          np.random.seed(42)\n",
    "          P = generate_initial_data(Bounds) # the X in the function is now P\n",
    "          P = np.unique(P,axis=0) # ensure that there are no repeated points in the initial dataset but no need to do this maybe? bc lhs is already generating unique points\n",
    "          # print(\"P\")\n",
    "          # print(P)\n",
    "          # print(\"P shape[0]\", P.shape[0])\n",
    "\n",
    "          while FE_ex < Max_FE_ex or FE_nex < Max_FE_nex:\n",
    "              # Step 2: Evaluate P on the most expensive objective function\n",
    "              F_exp,_,_ = evaluate_most_expensive_obj(P, Problem, id_ex) # F_exp = f, the FunctionValue, from evaluate_most_expensive_obj\n",
    "\n",
    "              if len(A_ex) == 0:\n",
    "                  A_ex = np.hstack((P, F_exp))\n",
    "              else:\n",
    "                  A_ex = np.vstack((A_ex, np.hstack((P, F_exp))))\n",
    "              # print(\"Aex\")\n",
    "              # print(A_ex) # A_ex的最後一個column是F_exp\n",
    "              # Step 3:\n",
    "              if latency > 1: # while f_ex(f2) is running, we can afford |P|*(latency-1) evaluations with f_nex(f1)\n",
    "                  if itr_count == 0: # change from 1 to 0 because the first iteration is 0 not 1 (modified: 0715)\n",
    "                      # single-objective EA to find solutions for f1 --> maximum no. of evaluations for runnign single-objective EA is |P|*(latency-1) --> purpose of useing single-obj EA is to find promising samples for \"training Kriging model of f1\" --> select training samples in step 5\n",
    "                      # single-objective EA only do once, because no. of evaluations available after the first iteration is not sfficient for single-objective optimisation (described in step 7?)\n",
    "                      X_nex, F_nex = optimize_least_expensive(P, Bounds, latency, Problem, id_nex) # this will call_GA --> simulated binary crossover and polynomial mutation\n",
    "                      F_nex = F_nex.reshape(-1,1)\n",
    "                      # print(\"F_nex in single-objectiv EA\")\n",
    "                      # print(F_nex)\n",
    "\n",
    "                  else:\n",
    "                      # crossover and mutation --> to generate samples and evaluate them with f1 (but crossover and mutation again???? already done in call_GA)\n",
    "                      X_nex, F_nex = genetic_operation(P, Bounds, latency, Problem, id_nex)\n",
    "                      F_nex = F_nex.reshape(-1,1)\n",
    "                      # print(\"F_nex in crossover and mutation\")\n",
    "                      # print(F_nex) # 只有 4 rows generated, 為何????\n",
    "\n",
    "              else:\n",
    "                  F_nex,_,_ = evaluate_least_expensive_obj(P, Problem, id_nex)\n",
    "                  F_nex = F_nex.reshape(-1, 1)\n",
    "                  X_nex = P\n",
    "\n",
    "              # add solutions evaluated with f1 to A_nex\n",
    "              if len(A_nex) == 0:\n",
    "                  A_nex = np.hstack((X_nex, F_nex))\n",
    "              else:\n",
    "                  A_nex = np.vstack((A_ex, np.hstack((X_nex, F_nex))))\n",
    "              # np.set_printoptions(threshold=np.inf)\n",
    "              # print(\"Anex\")\n",
    "              # print(A_nex) # A_nex的最後一column是F_nex\n",
    "\n",
    "\n",
    "              # Step 4: store all solutions evaluated by both objectives into A\n",
    "              # thus far, we have solutions evaluated with f1 and f2 in A_nex, A_ex\n",
    "              FE_ex += P.shape[0]\n",
    "              FE_nex += P.shape[0] * latency\n",
    "              # print(\"FE_ex after step 4:\", FE_ex)\n",
    "              # print(\"FE_nex after step 4:\", FE_nex)\n",
    "\n",
    "              # and now assign solutions evaluated with f1 and f2 (F_nex and F_exp) to A that has 4 columns [initial P for 2 objectives, F_nex, F_exp]\n",
    "              if len(A) == 0:\n",
    "                  A = select_solutions_for_archive(P, F_exp, F_nex, id_ex, id_nex)\n",
    "              else:\n",
    "                  A = np.vstack((A, select_solutions_for_archive(P, F_exp, F_nex, id_ex, id_nex)))\n",
    "              # print(\"A from select in algorithm\")\n",
    "              # print(A) # A 的左10是P 再來是F_nex 最後一column是F_exp\n",
    "\n",
    "              # Step 5: Build surrogate model (Kriging = GP) for both objective functions\n",
    "              model_ex = build_model(A_ex, no_var) # surrogate model for f2 --> A_ex assign給 build_model裡的A\n",
    "              model_nex = build_model(A_nex, no_var) # surrogate model for f1 --> A_nex assign給 build_model裡的A\n",
    "              # in surrogate model, we don't use same samples used in getting F_exp and F_nex for training the surrogate\n",
    "              # the size of A_nex > A_ex, so the no. of training samples for building surrogate models is differenrt for f1 and f2\n",
    "              # we train model_ex by using solutions(FunctionValue?) in A_ex; train model_nex by using solutions in A_nex\n",
    "              # but the no. of solutions in both A_ex and A_nex increase with the FE_ex and FE_nex\n",
    "              # therefore, we select predifined Max no. of samples (masFE = 1000 in evolve_k_rvea) --> this is set arbitrary, and needs to be adaptive, so this should be considered as future work\n",
    "              # so far, we have selected the training data and built the surrogates for f1 and f2\n",
    "\n",
    "\n",
    "              # Step 6: Run some surrogate-assisted algorithm (e.g., K-RVEA) to find the samples to be evaluated\n",
    "              # run a multiobjective EA (RVEA) to find smaples for updating the surrogates\n",
    "              # samples are selected to update surrogates with the strategy from K-RVEA, which selected samples based on diversity(uncertainty values obetained from the Kriging models) or convergence(APD)\n",
    "              # reference vectors are used for the needs of \"diversity\"\n",
    "              P,_ = run_K_RVEA(model_ex, model_nex, Bounds, A, id_ex, id_nex, empty_ref) # select sample P to be evlauated in next iteration's step 2 onward\n",
    "\n",
    "              itr_count += 1\n",
    "              # print(f\"finish {itr_count} iteration\")\n",
    "\n",
    "          # output (enter here after 1 run)\n",
    "          # print(\"FE_ex after 1 run\", FE_ex)\n",
    "          # print(\"FE_nex after 1 run\", FE_nex)\n",
    "          non = P_sort(A[:, no_var:], 'first') == 1  # A[:,no_var:] is actually same as 'FunctionValue'-->(nex's, ex's)\n",
    "          # print(\"non\",non)\n",
    "          PF_A = A[non, :] # nondominated solutions = Pareto Front\n",
    "          # print(f\"after {run+1} run's PF_A\")\n",
    "          # print(\"PF_A[:,no_var:]\")\n",
    "          # print(PF_A[:,no_var:])\n",
    "\n",
    "          max_nex = np.max(PF_A[:,no_var:][:,0])\n",
    "          min_nex = np.min(PF_A[:,no_var:][:,0])\n",
    "          max_ex = np.max(PF_A[:,no_var:][:,1])\n",
    "          min_ex = np.min(PF_A[:,no_var:][:,1])\n",
    "          max_nex_list.append(max_nex)\n",
    "          # print(f\"max list of f1 after {run+1} run of latency {latency}\")\n",
    "          # print(max_nex_list)\n",
    "          min_nex_list.append(min_nex)\n",
    "          # print(f\"min list of f1 after {run+1} run of latency {latency}\")\n",
    "          # print(min_nex_list)\n",
    "          max_ex_list.append(max_ex)\n",
    "          # print(f\"max list of f2 after {run+1} run of latency {latency}\")\n",
    "          # print(max_ex_list)\n",
    "          min_ex_list.append(min_ex)\n",
    "          # print(f\"min list of f2 after {run+1} run of latency {latency}\")\n",
    "          # print(min_ex_list)\n",
    "\n",
    "          # store the results of each run in a dictionary\n",
    "          latency_dict[f\"Run {run+1}\"]= PF_A[:,no_var:]\n",
    "          # print(\"latency_dict\")\n",
    "          # print(latency_dict)\n",
    "\n",
    "\n",
    "          # # Calculate median attainment\n",
    "          # distances_obj1 = np.linalg.norm(np.subtract(PF_A[:, no_var:][:, 0].reshape(-1,1),reference_point[0]))\n",
    "          # distances_obj2 = np.linalg.norm(np.subtract(PF_A[:, no_var:][:, 1].reshape(-1,1),reference_point[1]))\n",
    "\n",
    "          # hkrvea_median_attainment_obj1 = np.median(distances_obj1)\n",
    "          # hkrvea_median_attainment_obj2 = np.median(distances_obj2)\n",
    "\n",
    "          # hkrvea_at1.append(hkrvea_median_attainment_obj1)\n",
    "          # hkrvea_at2.append(hkrvea_median_attainment_obj2)\n",
    "          median_PF_A = PF_A[:,no_var:]\n",
    "          # print(\"median PF_A of each run\")\n",
    "          # print(median_PF_A)\n",
    "\n",
    "          if len(median_PF_A_all) == 0:\n",
    "            median_PF_A_all = median_PF_A\n",
    "          else:\n",
    "            median_PF_A_all = np.vstack((median_PF_A_all, median_PF_A))\n",
    "          # print(\"median PF_A_all for each latency, should have 21 rows and 2 columns in the end\")\n",
    "          # print(median_PF_A_all)\n",
    "          # print(\"median PF_A_All shape\",median_PF_A_all.shape)\n",
    "\n",
    "\n",
    "          # # Calculate median hypervolume\n",
    "          # ind = HV(ref_point=reference_point)\n",
    "          # hv = ind(PF_A[:,no_var:])\n",
    "          # hkrvea_hypervolumes.append(hv)\n",
    "          # print(f\"hypervolumes from latency {latency} after {run+1} runs:\", hkrvea_hypervolumes)\n",
    "          # print(\"count of hkrvea_hypervolumes:\", len(hkrvea_hypervolumes))\n",
    "\n",
    "          print(f\"Finish latency {latency}'s {run + 1} run\") # each run ends here\n",
    "\n",
    "    # will enter here after a latency finishes its 21 runs\n",
    "    solutions_dict[f\"Latency {latency}\"] = {'A': A[:,no_var:], 'PF_A': PF_A[:, no_var:]}\n",
    "    # print(\"solutions_dict, the results from last run of each latency\")\n",
    "    # print(solutions_dict)\n",
    "\n",
    "    median_pareto_front_dict[f'Latency {latency}'] = median_PF_A_all\n",
    "    # print('median_pareto_front_dict')\n",
    "    # print(median_pareto_front_dict)\n",
    "\n",
    "\n",
    "    results_dict[f\"Latency {latency}\"] = latency_dict   # 這個dictionary有所有latency及其21runs的pareto front --> 結構為'Latency _' -->'Run _' --> array(每run的pareto front)\n",
    "    # print(\"results_dict\")\n",
    "    # print(results_dict)\n",
    "    max_nex_all.append(np.max(max_nex_list))\n",
    "    # print(f\"max of f1 of latencies {Latencies}\")\n",
    "    # print(max_nex_all)\n",
    "    min_nex_all.append(np.min(min_nex_list))\n",
    "    # print(f\"min of f1 of latencies {Latencies}\")\n",
    "    # print(min_nex_all)\n",
    "    max_ex_all.append(np.max(max_ex_list))\n",
    "    # print(f\"max of f2 of latencies {Latencies}\")\n",
    "    # print(max_ex_all)\n",
    "    min_ex_all.append(np.min(min_ex_list))\n",
    "    # print(f\"min of f2 of latencies {Latencies}\")\n",
    "    # print(min_ex_all)\n",
    "\n",
    "#     hkrvea_median_hypervolume = np.median(hkrvea_hypervolumes)\n",
    "#     hkrvea_hv.append(hkrvea_median_hypervolume)\n",
    "#     print(f\"hkrvea_hv (should have {len(Latencies)} median hv for latencies {Latencies}):\",hkrvea_hv)\n",
    "# for i, lat in enumerate(Latencies):\n",
    "#   print(f\"Median Hypervolume for latency {lat}: {hkrvea_hv[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvHqLNu2wABv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvHqLNu2wABv",
    "outputId": "9077dd51-e176-4df3-d2c2-ee1404b78392"
   },
   "outputs": [],
   "source": [
    "# will enter here after all latencies finish their 21 runs (after the whole algorithm finish)\n",
    "overall_max_nex = np.max(max_nex_all)\n",
    "# print(f\"overall max for f1 of latencies {Latencies}\")\n",
    "# print(overall_max_nex)\n",
    "\n",
    "overall_min_nex = np.min(min_nex_all)\n",
    "# print(f\"overall min for f1 of latency {Latencies}\")\n",
    "# print(overall_min_nex)\n",
    "\n",
    "overall_max_ex = np.max(max_ex_all)\n",
    "# print(f\"overall max for f2 of latency {Latencies}\")\n",
    "# print(overall_max_ex)\n",
    "\n",
    "overall_min_ex = np.max(min_ex_all)\n",
    "# print(f\"overall min for f2 of latency {Latencies}\")\n",
    "# print(overall_min_ex)\n",
    "\n",
    "max_values = np.array([overall_max_nex, overall_max_ex])\n",
    "min_values = np.array([overall_min_nex, overall_min_ex])\n",
    "\n",
    "# save pareto fronts across all latencies and runs to 'results_dict'\n",
    "for latency_key in results_dict.keys():\n",
    "  runs_dict = results_dict[latency_key]\n",
    "  for run_key in runs_dict.keys():\n",
    "    PF = runs_dict[run_key]\n",
    "print(\"original PF_A\")\n",
    "print(results_dict)\n",
    "\n",
    "# normalise all the pareto fronts and save them to 'results_dict_norm'\n",
    "for latency_key in results_dict.keys():\n",
    "  runs_dict = results_dict[latency_key]\n",
    "  norm_runs_dict = {}\n",
    "  for run_key in runs_dict.keys():\n",
    "    PF = runs_dict[run_key]\n",
    "    # print(\"PF from results dict {run_key}\", PF)\n",
    "    PF_A_norm = (max_values - PF)/ (max_values - min_values)\n",
    "    # print(\"norm PF\", PF_A)\n",
    "    norm_runs_dict[run_key] = PF_A_norm\n",
    "  results_dict_norm[latency_key] = norm_runs_dict\n",
    "print(\"normalised PF_A\")\n",
    "print(results_dict_norm)\n",
    "\n",
    "\n",
    "# calculate hypervolume\n",
    "hkrvea_hypervolumes_median=[]\n",
    "for latency_key in results_dict_norm.keys():\n",
    "  runs_dict = results_dict_norm[latency_key]\n",
    "  hkrvea_hypervolumes = []\n",
    "\n",
    "  for run_key in runs_dict.keys():\n",
    "    PF = runs_dict[run_key]\n",
    "    # print(\"PF from {latency_key} {run_key} norm\",PF)\n",
    "    ind = HV(ref_point = reference_point)\n",
    "    hv = ind(PF)\n",
    "    hv = hv / np.prod(reference_point) # if set reference_point to between 1 and 2 for DTLZ, there is no need to execute this line\n",
    "    # print(f\"hv of {latency_key} {run_key}\", hv)\n",
    "\n",
    "    hkrvea_hypervolumes.append(hv)\n",
    "  print(f\"hypervolumes for {latency_key}\",hkrvea_hypervolumes)\n",
    "\n",
    "  hkrvea_median_hypervolume = np.median(hkrvea_hypervolumes)\n",
    "  hkrvea_hypervolumes_median.append(hkrvea_median_hypervolume)\n",
    "  print(f\"median hypervoluems for {latency_key}: {hkrvea_median_hypervolume}\")\n",
    "  print(\"hkrvea_hypervolumes_median\",hkrvea_hypervolumes_median)\n",
    "  print(\"------------\")\n",
    "  # print(f\"hypervoluems from latency {latency_key}:\")\n",
    "  # print(hkrvea_hypervolumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627115fe-2ce6-4d05-92eb-fd8f0b951d3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Reuslts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad08c3-73eb-4224-a5ac-20a7d3e4ece7",
   "metadata": {
    "id": "8aad08c3-73eb-4224-a5ac-20a7d3e4ece7"
   },
   "source": [
    "### Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce680c13-492b-485f-90ab-7dd58566a592",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ce680c13-492b-485f-90ab-7dd58566a592",
    "outputId": "039929d8-de4f-48eb-8f69-6735b6297afc"
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# # the FunctionValue is from archive A's right 2 columns (FunctionValue of f_nex, FunctionValue of f_ex)\n",
    "\n",
    "def plot_latency(latency, A, PF_A):\n",
    "    # Scatter plot of all solutions (color: blue)\n",
    "    plt.scatter(A[:, 0], A[:, 1], label='All solutions', s=20, c='blue') # 0802從no_var:跟no_var+1改成0,1\n",
    "\n",
    "    # Scatter plot of nondominated solutions (color: red)\n",
    "    plt.scatter(PF_A[:, 0], PF_A[:, 1], label='Nondominated solutions', s=20, c='red')\n",
    "\n",
    "    plt.title(\"{}, {}\".format(Problem, latency))\n",
    "    plt.legend()\n",
    "    plt.xlabel('cheap function f1')\n",
    "    plt.ylabel('expensive function f2')\n",
    "\n",
    "    # Save the data used to generate the plot\n",
    "    np.save(os.path.join(problem_directory, f'{Problem} {no_var}var {latency} A_latency_{latency}.npy'), A)\n",
    "    np.save(os.path.join(problem_directory, f'{Problem} {no_var}var {latency} PF_A_latency_{latency}.npy'), PF_A)\n",
    "\n",
    "    # Save the plot as an image (optional)\n",
    "    plt.savefig(os.path.join(problem_directory, f'{Problem} {no_var}var {latency} Pareto_Front_plot.png'))\n",
    "    plt.show()\n",
    "    plt.close()  # Close the current plot to avoid overlapping plots\n",
    "\n",
    "# Iterate through the solutions_dict and plot for each latency\n",
    "for latency, data in solutions_dict.items():\n",
    "    A = data['A']\n",
    "    PF_A = data['PF_A']\n",
    "    plot_latency(latency, A, PF_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MhZ5t_OO7Pxs",
   "metadata": {
    "id": "MhZ5t_OO7Pxs"
   },
   "source": [
    "### Hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sfzjRcEUqLN6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "sfzjRcEUqLN6",
    "outputId": "bb4507c2-8339-448a-fd8a-49f79019aaab"
   },
   "outputs": [],
   "source": [
    "# line plot of median hypervolume\n",
    "plt.figure(figsize=(8, 6))\n",
    "# plt.plot(Latencies, hkrvea_hv,marker = 'o')\n",
    "plt.plot(Latencies, hkrvea_hypervolumes_median, label = 'HK-RVEA', marker = 'o')\n",
    "plt.legend()\n",
    "plt.xlabel('Latency')\n",
    "plt.ylabel('hv')\n",
    "plt.title(f'{Problem} Median Hypervolume')\n",
    "plt.xticks(Latencies)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.ylim(0,1)\n",
    "x_positions = range(len(Latencies))\n",
    "plt.savefig(os.path.join(problem_directory, f'{Problem} {no_var}var Median Hypervolume.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
